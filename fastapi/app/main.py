from fastapi import FastAPI, Query, UploadFile, File, Form, HTTPException, Request
from db import init_db, save_message, fetch_messages
from storage import upload_file_to_minio, read_text_from_minio
import requests
import os
import json
import openai
import psycopg2
from psycopg2.extras import RealDictCursor
from dotenv import load_dotenv

# ======================================================
# üöÄ CONFIGURACI√ìN INICIAL
# ======================================================
app = FastAPI(title="Ripsy Chatbot - MVP")
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

OLLAMA_URL = os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

SYSTEM_PROMPT_KEY = os.getenv("SYSTEM_PROMPT_KEY", "config/prompt_ripsy.txt")
GLOSSARY_KEY = os.getenv("GLOSSARY_KEY", "config/glosario_salud.txt")

DEFAULT_PROMPT_FALLBACK = (
    "Eres Ripsy üíô, un asistente experto en facturaci√≥n en salud en Colombia. "
    "Responde √∫nicamente sobre RIPS, radicaci√≥n de facturas, auditor√≠a y glosas. "
    "Si la pregunta no es de salud, responde: "
    "\"Lo siento, solo puedo responder sobre temas de facturaci√≥n en salud en Colombia.\""
)

SYSTEM_PROMPT = DEFAULT_PROMPT_FALLBACK
GLOSSARY_TEXT = ""


# ======================================================
# üîπ FUNCIONES UTILES
# ======================================================
def chunk_text(text: str, max_chars: int = 3500):
    """
    Divide texto en fragmentos peque√±os (√∫til para glosarios largos)
    """
    text = (text or "").strip()
    if not text:
        return []
    return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]


# ======================================================
# üîπ EVENTO DE INICIO
# ======================================================
@app.on_event("startup")
def on_startup():
    """
    Al iniciar FastAPI:
    - Inicializa la base de datos
    - Carga el prompt y el glosario desde MinIO
    """
    init_db()
    global SYSTEM_PROMPT, GLOSSARY_TEXT

    try:
        SYSTEM_PROMPT = read_text_from_minio(SYSTEM_PROMPT_KEY)
        print(f"‚úÖ Prompt cargado ({len(SYSTEM_PROMPT)} caracteres)")
    except Exception as e:
        SYSTEM_PROMPT = DEFAULT_PROMPT_FALLBACK
        print(f"‚ö†Ô∏è Error cargando prompt, usando fallback: {e}")

    try:
        GLOSSARY_TEXT = read_text_from_minio(GLOSSARY_KEY)
        print(f"‚úÖ Glosario cargado ({len(GLOSSARY_TEXT)} caracteres)")
    except Exception as e:
        GLOSSARY_TEXT = ""
        print(f"‚ö†Ô∏è No se pudo leer glosario: {e}")


# ======================================================
# üîπ ENDPOINTS PRINCIPALES
# ======================================================
@app.get("/")
def read_root():
    return {"ok": True, "message": "Hola, soy Ripsy üíô. Tu chatbot est√° corriendo."}


# ======================================================
# üîπ CHAT OPENAI
# ======================================================
from llm import generate_reply, test_openai_connection

@app.post("/chat")
def chat(payload: dict):
    user = payload.get("user", "desconocido")
    message = payload.get("message", "")
    if not message:
        raise HTTPException(status_code=400, detail="Falta 'message' en el payload")

    history = fetch_messages(limit=10)
    reply = generate_reply(user, message, history)
    save_message(user, message, reply)
    return {"ok": True, "user": user, "respuesta": reply}


@app.get("/test-openai")
def test_openai():
    return test_openai_connection()


# ======================================================
# üîπ CHAT LLAMA (modelo local)
# ======================================================
@app.post("/chat-llama")
def chat_llama(payload: dict):
    user = payload.get("user", "desconocido")
    message = payload.get("message", "")
    if not message:
        raise HTTPException(status_code=400, detail="Falta 'message' en el payload")

    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    if GLOSSARY_TEXT:
        for idx, chunk in enumerate(chunk_text(GLOSSARY_TEXT, 3500), start=1):
            messages.append({
                "role": "system",
                "content": f"üìö Glosario de salud (fragmento {idx}):\n{chunk}"
            })
    messages.append({"role": "user", "content": message})

    try:
        resp = requests.post(
            f"{OLLAMA_URL}/api/chat",
            json={"model": "llama3", "messages": messages, "options": {"num_ctx": 4096}},
            stream=True, timeout=30
        )
        resp.raise_for_status()

        reply_parts = []
        for line in resp.iter_lines():
            if line:
                try:
                    data = json.loads(line.decode("utf-8"))
                    if "message" in data and "content" in data["message"]:
                        reply_parts.append(data["message"]["content"])
                except json.JSONDecodeError:
                    continue
        reply = "".join(reply_parts).strip() or "Lo siento, no recib√≠ respuesta de Llama3."
    except Exception as e:
        reply = f"‚ùå Error con Llama3: {e}"

    save_message(user, message, reply)
    return {"ok": True, "user": user, "respuesta": reply}


# ======================================================
# üß† ENDPOINT RAG: CONSULTAR NORMAS
# ======================================================
@app.post("/consultar-normas")
async def consultar_normas(request: Request):
    """
    Usa embeddings para buscar y responder bas√°ndose en las normas vectorizadas.
    """
    body = await request.json()
    pregunta = body.get("message")

    if not pregunta:
        raise HTTPException(status_code=400, detail="Debe incluir 'message' en el body")

    # 1Ô∏è‚É£ Crear embedding de la pregunta
    emb_res = openai.embeddings.create(
        model="text-embedding-3-small",
        input=pregunta
    )
    emb_vector = emb_res.data[0].embedding

    # 2Ô∏è‚É£ Buscar los fragmentos m√°s parecidos
    conn = psycopg2.connect(
        host=os.getenv("POSTGRES_HOST"),
        port=os.getenv("POSTGRES_PORT"),
        dbname=os.getenv("POSTGRES_DB"),
        user=os.getenv("POSTGRES_USER"),
        password=os.getenv("POSTGRES_PASSWORD"),
        cursor_factory=RealDictCursor
    )
    cur = conn.cursor()
    cur.execute("""
        SELECT filename, chunk,
               1 - (embedding <=> %s::vector) AS similarity
        FROM normativas_embeddings
        ORDER BY embedding <=> %s::vector
        LIMIT 5;
    """, (emb_vector, emb_vector))
    resultados = cur.fetchall()
    conn.close()

    contexto = "\n".join([r["chunk"] for r in resultados])

    # 3Ô∏è‚É£ Generar respuesta con contexto
    prompt = f"""
    Eres Ripsy üíô, asistente especializado en facturaci√≥n y normatividad en salud.
    Usa los siguientes fragmentos normativos para responder con precisi√≥n y referencias cuando sea posible.

    Contexto normativo:
    {contexto}

    Pregunta:
    {pregunta}
    """

    completion = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=600,
        temperature=0.3
    )

    respuesta = completion.choices[0].message.content
    return {
        "ok": True,
        "pregunta": pregunta,
        "respuesta": respuesta,
        "documentos_usados": [r["filename"] for r in resultados]
    }


# ======================================================
# üîπ RECARGAR PROMPT / GLOSARIO
# ======================================================
@app.post("/config/reload-prompt")
def reload_prompt():
    global SYSTEM_PROMPT
    SYSTEM_PROMPT = read_text_from_minio(SYSTEM_PROMPT_KEY)
    return {"ok": True, "chars": len(SYSTEM_PROMPT)}

@app.post("/config/reload-glossary")
def reload_glossary():
    global GLOSSARY_TEXT
    GLOSSARY_TEXT = read_text_from_minio(GLOSSARY_KEY)
    return {"ok": True, "chars": len(GLOSSARY_TEXT)}

@app.post("/config/reload")
def reload_both():
    global SYSTEM_PROMPT, GLOSSARY_TEXT
    SYSTEM_PROMPT = read_text_from_minio(SYSTEM_PROMPT_KEY)
    GLOSSARY_TEXT = read_text_from_minio(GLOSSARY_KEY)
    return {
        "ok": True,
        "prompt_chars": len(SYSTEM_PROMPT),
        "glossary_chars": len(GLOSSARY_TEXT)
    }


# ======================================================
# üîπ HISTORIAL DE MENSAJES
# ======================================================
@app.get("/messages")
def get_messages(limit: int = Query(20, ge=1, le=200)):
    return {"count": limit, "items": fetch_messages(limit)}


# ======================================================
# üîπ DOCUMENTOS (subir / listar)
# ======================================================
@app.post("/documents/upload")
def upload_document(file: UploadFile = File(...), folder: str = Form("facturas")):
    if not file.filename:
        raise HTTPException(status_code=400, detail="Archivo sin nombre")

    allowed = {'.pdf', '.txt', '.doc', '.docx', '.xls', '.xlsx', '.json', '.xml'}
    ext = os.path.splitext(file.filename)[1].lower()
    if ext not in allowed:
        raise HTTPException(status_code=400, detail=f"Extensi√≥n no permitida: {ext}")

    file.file.seek(0, 2)
    size = file.file.tell()
    file.file.seek(0)
    if size > 10 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="Archivo demasiado grande (m√°x 10MB)")

    result = upload_file_to_minio(file.file, file.filename, folder)
    return {"ok": True, "folder": folder, "file": file.filename, "message": result}


@app.get("/documents/list")
def list_documents(folder: str = Query("facturas")):
    from storage import list_files_in_folder
    files = list_files_in_folder(folder)
    return {"ok": True, "folder": folder, "files": files}
