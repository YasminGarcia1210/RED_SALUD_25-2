# üöÄ Pipeline de Facturaci√≥n RIPS - Versi√≥n Modernizada

Pipeline automatizado con **Prefect** y **PySpark** para procesar facturas e historias electr√≥nicas en salud (HEV) y generar archivos RIPS JSON seg√∫n la normativa colombiana.

## ‚ú® Nuevas Caracter√≠sticas

### üîß **Arquitectura Modernizada**
- **Prefect**: Orquestaci√≥n y monitoreo de flujos de trabajo
- **PySpark**: Procesamiento distribuido y escalable
- **Pydantic**: Configuraci√≥n tipada y validaci√≥n
- **Logging estructurado**: Trazabilidad completa del proceso

### üìä **Funcionalidades Avanzadas**
- ‚úÖ **Validaci√≥n robusta** de archivos RIPS seg√∫n normativa
- ‚úÖ **Procesamiento distribuido** con PySpark
- ‚úÖ **Monitoreo en tiempo real** con Prefect
- ‚úÖ **CLI interactiva** con Rich para mejor UX
- ‚úÖ **Reportes autom√°ticos** de validaci√≥n y m√©tricas
- ‚úÖ **Manejo de errores** y reintentos autom√°ticos
- ‚úÖ **Configuraci√≥n centralizada** por entorno

## üèóÔ∏è Arquitectura del Proyecto

```
pipeline_facturacion/
‚îú‚îÄ‚îÄ üìÅ config/
‚îÇ   ‚îî‚îÄ‚îÄ settings.py              # Configuraci√≥n centralizada
‚îú‚îÄ‚îÄ üìÅ utils/
‚îÇ   ‚îî‚îÄ‚îÄ logger.py                # Sistema de logging estructurado
‚îú‚îÄ‚îÄ üìÅ validation/
‚îÇ   ‚îî‚îÄ‚îÄ rips_validator.py        # Validador robusto de RIPS
‚îú‚îÄ‚îÄ üìÅ processing/
‚îÇ   ‚îî‚îÄ‚îÄ spark_processor.py       # Procesador distribuido con PySpark
‚îú‚îÄ‚îÄ üìÅ pipeline/
‚îÇ   ‚îî‚îÄ‚îÄ main_pipeline.py         # Pipeline principal con Prefect
‚îú‚îÄ‚îÄ üìÅ input/
‚îÇ   ‚îú‚îÄ‚îÄ fact_pdf/                # Facturas en PDF
‚îÇ   ‚îú‚îÄ‚îÄ fact_xml/                # Facturas electr√≥nicas XML
‚îÇ   ‚îî‚îÄ‚îÄ hev/                     # Historias cl√≠nicas electr√≥nicas
‚îú‚îÄ‚îÄ üìÅ output/
‚îÇ   ‚îî‚îÄ‚îÄ rips/                    # RIPS JSON generados
‚îú‚îÄ‚îÄ üìÅ control/                  # Reportes de control
‚îú‚îÄ‚îÄ üìÅ logs/                     # Logs estructurados
‚îú‚îÄ‚îÄ cli.py                       # Interfaz de l√≠nea de comandos
‚îú‚îÄ‚îÄ requirements.txt             # Dependencias actualizadas
‚îî‚îÄ‚îÄ README.md                    # Este archivo
```

## üöÄ Instalaci√≥n y Configuraci√≥n

### 1. **Instalar Dependencias**

```bash
# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate     # Windows

# Instalar dependencias
pip install -r requirements.txt
```

### 2. **Configurar Variables de Entorno**

Crear archivo `.env` en la ra√≠z del proyecto:

```env
# Configuraci√≥n del pipeline
BATCH_SIZE=100
LOG_LEVEL=INFO
LOG_FORMAT=json
MAX_RETRIES=3
TIMEOUT_SECONDS=300

# Configuraci√≥n de Spark
SPARK_APP_NAME=RIPS-Pipeline
SPARK_MASTER=local[*]
SPARK_DRIVER_MEMORY=2g
SPARK_EXECUTOR_MEMORY=2g
SPARK_MAX_WORKERS=4

# Configuraci√≥n de Prefect
PREFECT_API_URL=http://localhost:4200/api
PREFECT_PROJECT=rips-pipeline
PREFECT_WORK_QUEUE=rips-queue

# Notificaciones (opcional)
ENABLE_NOTIFICATIONS=false
NOTIFICATION_WEBHOOK=
```

### 3. **Preparar Estructura de Directorios**

```bash
# Crear directorios necesarios
mkdir -p input/fact_pdf input/fact_xml input/hev
mkdir -p output/rips control logs
```

## üéØ Uso del Pipeline

### **Interfaz de L√≠nea de Comandos (CLI)**

#### **Ejecutar Pipeline Completo**
```bash
# Ejecuci√≥n b√°sica
python -m pipeline_facturacion.cli run

# Con opciones avanzadas
python -m pipeline_facturacion.cli run \
  --environment production \
  --notifications \
  --output-format table
```

#### **Procesamiento por Lotes**
```bash
# Procesar en lotes de 50 archivos
python -m pipeline_facturacion.cli batch --batch-size 50
```

#### **Verificar Configuraci√≥n**
```bash
# Mostrar configuraci√≥n actual
python -m pipeline_facturacion.cli config --environment development
```

#### **Verificar Estructura**
```bash
# Verificar directorios y archivos
python -m pipeline_facturacion.cli check --path .
```

#### **Validar Archivos RIPS**
```bash
# Validar archivos RIPS existentes
python -m pipeline_facturacion.cli validate --environment development
```

### **Uso Program√°tico**

```python
from pipeline_facturacion.pipeline.main_pipeline import rips_pipeline

# Ejecutar pipeline
result = rips_pipeline(
    environment="development",
    enable_notifications=False
)

print(f"Resultado: {result}")
```

## üìä Flujo del Pipeline

### **1. Descubrimiento de Archivos**
- Escanea autom√°ticamente las carpetas de entrada
- Valida existencia y formato de archivos
- Genera inventario de archivos a procesar

### **2. Procesamiento Distribuido**
- **PySpark**: Procesa archivos HEV y XML en paralelo
- **UDFs personalizadas**: Extracci√≥n de datos con regex optimizadas
- **Join distribuido**: Consolida informaci√≥n de m√∫ltiples fuentes

### **3. Validaci√≥n Robusta**
- **Validaci√≥n de estructura**: Campos obligatorios y tipos de datos
- **Validaci√≥n de negocio**: C√≥digos CUPS, diagn√≥sticos, fechas
- **Reportes detallados**: Errores y advertencias por archivo

### **4. Generaci√≥n de RIPS**
- **Estructura JSON**: Conforme a normativa colombiana
- **Validaci√≥n CUV**: C√≥digos √∫nicos de validaci√≥n
- **Archivos individuales**: Un RIPS por factura

### **5. Monitoreo y Reportes**
- **M√©tricas en tiempo real**: Tiempo de procesamiento, tasas de √©xito
- **Logs estructurados**: Trazabilidad completa del proceso
- **Reportes autom√°ticos**: Res√∫menes y estad√≠sticas

## üîç Validaciones Implementadas

### **Validaciones de Estructura**
- ‚úÖ Campos obligatorios del RIPS
- ‚úÖ Tipos de datos correctos
- ‚úÖ Estructura JSON v√°lida

### **Validaciones de Negocio**
- ‚úÖ N√∫mero de factura (formato FERO + 6 d√≠gitos)
- ‚úÖ NIT del obligado (9-10 d√≠gitos)
- ‚úÖ Tipos de documento v√°lidos (CC, TI, RC, etc.)
- ‚úÖ C√≥digos CUPS v√°lidos para vacunaci√≥n
- ‚úÖ Diagn√≥sticos (excluyendo Z00-Z99)
- ‚úÖ Fechas en formato correcto
- ‚úÖ Valores de servicios > 0

### **Validaciones de Calidad**
- ‚úÖ Detecci√≥n de inasistencias
- ‚úÖ Coherencia entre facturas y registros cl√≠nicos
- ‚úÖ Validaci√≥n de servicios prestados

## üìà M√©tricas y Monitoreo

### **M√©tricas Autom√°ticas**
- **Tiempo de procesamiento** por archivo y lote
- **Tasa de √©xito** de validaci√≥n
- **Tasa de error** por tipo de error
- **Throughput** de archivos por minuto

### **Reportes Generados**
- `summary_report_YYYYMMDD_HHMMSS.json`: Resumen del procesamiento
- `metrics_report_YYYYMMDD_HHMMSS.json`: M√©tricas detalladas
- `validation_report_YYYYMMDD_HHMMSS.json`: Resultados de validaci√≥n

## üõ†Ô∏è Configuraci√≥n Avanzada

### **Configuraci√≥n de Spark**
```python
# En .env o variables de entorno
SPARK_MASTER=yarn                    # Para cluster
SPARK_DRIVER_MEMORY=4g              # M√°s memoria para procesamiento
SPARK_EXECUTOR_MEMORY=4g
SPARK_MAX_WORKERS=8                 # M√°s workers para paralelismo
```

### **Configuraci√≥n de Prefect**
```python
# Habilitar UI de Prefect
prefect server start

# Configurar work queue
prefect work-queue create rips-queue
```

### **Configuraci√≥n de Logging**
```python
# Logs en formato JSON para an√°lisis
LOG_FORMAT=json
LOG_LEVEL=DEBUG  # Para desarrollo
LOG_LEVEL=WARNING  # Para producci√≥n
```

## üß™ Testing y Validaci√≥n

### **Ejecutar Tests**
```bash
# Instalar dependencias de testing
pip install pytest pytest-asyncio

# Ejecutar tests
pytest tests/
```

### **Validar Archivos de Ejemplo**
```bash
# Validar archivos RIPS existentes
python -m pipeline_facturacion.cli validate

# Verificar estructura
python -m pipeline_facturacion.cli check
```

## üìã Tareas Pendientes y Mejoras

### **Mejoras Implementadas** ‚úÖ
- [x] Arquitectura con Prefect y PySpark
- [x] Configuraci√≥n centralizada con Pydantic
- [x] Logging estructurado
- [x] Validaci√≥n robusta de RIPS
- [x] CLI interactiva con Rich
- [x] Procesamiento distribuido
- [x] Reportes autom√°ticos
- [x] Manejo de errores mejorado

### **Pr√≥ximas Mejoras** üîÑ
- [ ] **Integraci√≥n con bases de datos**: PostgreSQL/MongoDB
- [ ] **API REST**: Endpoints para monitoreo
- [ ] **Dashboard web**: Interfaz gr√°fica
- [ ] **Notificaciones avanzadas**: Email, Slack, Teams
- [ ] **CI/CD**: Pipeline de despliegue automatizado
- [ ] **Tests unitarios**: Cobertura completa
- [ ] **Documentaci√≥n API**: OpenAPI/Swagger

## üÜò Soluci√≥n de Problemas

### **Errores Comunes**

#### **Error: No se encuentran archivos**
```bash
# Verificar estructura
python -m pipeline_facturacion.cli check

# Verificar archivos en carpetas de entrada
ls input/hev/
ls input/fact_xml/
```

#### **Error: Spark no inicia**
```bash
# Verificar configuraci√≥n de Spark
python -m pipeline_facturacion.cli config

# Usar configuraci√≥n local
export SPARK_MASTER=local[*]
```

#### **Error: Prefect no conecta**
```bash
# Iniciar servidor Prefect
prefect server start

# Verificar configuraci√≥n
python -m pipeline_facturacion.cli config
```

### **Logs y Debugging**
```bash
# Ver logs en tiempo real
tail -f logs/pipeline.log

# Cambiar nivel de log
export LOG_LEVEL=DEBUG
```

## üìû Soporte

### **Comandos de Ayuda**
```bash
# Ayuda general
python -m pipeline_facturacion.cli --help

# Ayuda espec√≠fica
python -m pipeline_facturacion.cli run --help
python -m pipeline_facturacion.cli validate --help
```

### **Informaci√≥n del Sistema**
```bash
# Verificar configuraci√≥n
python -m pipeline_facturacion.cli config

# Verificar estructura
python -m pipeline_facturacion.cli check
```

## üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Ver archivo `LICENSE` para m√°s detalles.

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---
